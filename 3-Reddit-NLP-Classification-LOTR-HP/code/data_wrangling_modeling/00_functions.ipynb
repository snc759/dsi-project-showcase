{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Natural Language Processing (NLP) Pre-processing/cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_words_only(text):\n",
    "    \"\"\"Cleans text so that any special characters are removed and only words are kept\"\"\"\n",
    "    \n",
    "    regex_tokenizer = RegexpTokenizer(\"\\w+\")\n",
    "    words = regex_tokenizer.tokenize(text.lower())\n",
    "    \n",
    "    return (' ').join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_stem(text):\n",
    "    \"\"\"Cleans text by keeping words only, tokenizing, stemming and removing stopwords\"\"\"\n",
    "    #Instantiate tokenizer and stemmer and lemmatizer\n",
    "    re_tokenizer = RegexpTokenizer(\"\\w+\")\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    p_stemmer = PorterStemmer()\n",
    "        \n",
    "    # Tokenze the text\n",
    "    words = re_tokenizer.tokenize(text.lower())\n",
    "    \n",
    "    # Filter stop words\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    \n",
    "    # Adds names to stopwords_list\n",
    "#     names = ['harry', 'potter', 'hp', 'lotr', 'tolkien']\n",
    "#     stopwords_list.append(names)\n",
    "\n",
    "    no_stops_stemmed = [p_stemmer.stem(word) for word in words if word.lower() not in stopwords_list]\n",
    "    \n",
    "    return (' ').join(no_stops_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_lem(text):\n",
    "    \"\"\"Cleans text by keeping words only, tokenizing, lemmatizing and removing stopwords\"\"\"\n",
    "    #Instantiate tokenizer and stemmer and lemmatizer\n",
    "    re_tokenizer = RegexpTokenizer(\"\\w+\")\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    p_stemmer = PorterStemmer()\n",
    "        \n",
    "    # Tokenze the text\n",
    "    words = re_tokenizer.tokenize(text.lower())\n",
    "    \n",
    "    # Filter stop words\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    \n",
    "    # Adds names to stopwords_list\n",
    "#     names = ['harry', 'potter', 'hp', 'lotr', 'tolkien']\n",
    "#     stopwords_list.append(names)\n",
    "\n",
    "    no_stops_lemmatized = [lemmatizer.lemmatize(word) for word in words if word.lower() not in stopwords_list]\n",
    "    \n",
    "    return (' ').join(no_stops_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove special chars\n",
    "df['title'] = df['title'].replace('[^\\w ]','',regex=True).astype(str) \n",
    "# awesome regex shortcut thanks to https://stackoverflow.com/questions/1219915/regex-to-remove-apostrophe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, text_col):\n",
    "    \n",
    "    df[text_col] = df[text_col].replace(['[removed]', '[deleted]'], np.nan)  # THis is done if bottom two commented out are important\n",
    "    \n",
    "#     df.insert(4, 'title_selftext', df['title'] + ' ' + df['selftext'])\n",
    "#     df['title_selftext'].fillna(df['title'], inplace=True)\n",
    "    \n",
    "    df.dropna(subset=[text_col], inplace=True)\n",
    "    df[text_col] = df[text_col].map(keep_words_only)\n",
    "    df['text_length'] = df[text_col].map(len)\n",
    "    df = df[df[text_col].map(len) > 10]\n",
    "    df.drop_duplicates(subset=[text_col], inplace=True)\n",
    "    \n",
    "    df['clean_text_stem'] = df[text_col].map(clean_text_stem)\n",
    "    df['clean_text_lem'] = df[text_col].map(clean_text_lem)\n",
    "    \n",
    "    df = df[df['clean_text_stem'].map(len) > 10]\n",
    "    df = df[df['clean_text_lem'].map(len) > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
