{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling with Stemmed and Balanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "\n",
    "# standard sklearn imports\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# tensorflow imports for Neural Networks\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, Flatten, Conv2D, MaxPooling2D, GRU, LSTM, Embedding, Bidirectional\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Import regularizers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "# Import Dropout\n",
    "from tensorflow.keras.layers import Dropout\n",
    "# Import Early Stopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "\n",
    "# CNN imports \n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "# GridSearch imports \n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# RNN imports \n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# imports for reports on classification\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "\n",
    "plt.style.use(style='seaborn')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Prepping the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>name</th>\n",
       "      <th>review_id</th>\n",
       "      <th>review_stars</th>\n",
       "      <th>text</th>\n",
       "      <th>amb_casual</th>\n",
       "      <th>amb_classy</th>\n",
       "      <th>amb_target</th>\n",
       "      <th>text_length</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_text_length</th>\n",
       "      <th>clean_text_stem</th>\n",
       "      <th>clean_text_stem_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0lCiLKpjrinltPFbBby4sw</td>\n",
       "      <td>The Great Wall Restaurant</td>\n",
       "      <td>wve8w6gIuPpCfo5J--AHjg</td>\n",
       "      <td>3</td>\n",
       "      <td>The menu sounded promising, with over fifty di...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>121</td>\n",
       "      <td>menu sounded promising fifty different dishes ...</td>\n",
       "      <td>68</td>\n",
       "      <td>menu sound promis fifti differ dish differ sty...</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0lCiLKpjrinltPFbBby4sw</td>\n",
       "      <td>The Great Wall Restaurant</td>\n",
       "      <td>5rFuHGGbimVxPHxgM0sNSA</td>\n",
       "      <td>3</td>\n",
       "      <td>This wasn't the worst Chinese food but it wasn...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "      <td>wasn' worst chinese food wasn' best egg foo yo...</td>\n",
       "      <td>41</td>\n",
       "      <td>worst chines food best egg foo young dri overc...</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0lCiLKpjrinltPFbBby4sw</td>\n",
       "      <td>The Great Wall Restaurant</td>\n",
       "      <td>2iD3Rdbw0DUzjZSqBq3hXQ</td>\n",
       "      <td>1</td>\n",
       "      <td>I have been coming to this restaurant for over...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>coming restaurant 20 years purchased shrimp fr...</td>\n",
       "      <td>27</td>\n",
       "      <td>come restaur 20 year purchas shrimp fri rice g...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0lCiLKpjrinltPFbBby4sw</td>\n",
       "      <td>The Great Wall Restaurant</td>\n",
       "      <td>e61y5ZlNwg04mAGtcD3vbQ</td>\n",
       "      <td>5</td>\n",
       "      <td>My husband and I love this place.\\nGreat price...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>husband love place great price lot food make s...</td>\n",
       "      <td>13</td>\n",
       "      <td>husband love place great price lot food make s...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kZFTi8FKjs30EuzurZ3v3g</td>\n",
       "      <td>Donerick's Pub</td>\n",
       "      <td>38lN2ONaypsfBDLwhGxcSg</td>\n",
       "      <td>5</td>\n",
       "      <td>Great place for beverages with your friends wh...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>great place beverages friends watch game lots ...</td>\n",
       "      <td>43</td>\n",
       "      <td>great place beverag friend watch game lot tv g...</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                       name               review_id  \\\n",
       "0  0lCiLKpjrinltPFbBby4sw  The Great Wall Restaurant  wve8w6gIuPpCfo5J--AHjg   \n",
       "1  0lCiLKpjrinltPFbBby4sw  The Great Wall Restaurant  5rFuHGGbimVxPHxgM0sNSA   \n",
       "2  0lCiLKpjrinltPFbBby4sw  The Great Wall Restaurant  2iD3Rdbw0DUzjZSqBq3hXQ   \n",
       "3  0lCiLKpjrinltPFbBby4sw  The Great Wall Restaurant  e61y5ZlNwg04mAGtcD3vbQ   \n",
       "4  kZFTi8FKjs30EuzurZ3v3g             Donerick's Pub  38lN2ONaypsfBDLwhGxcSg   \n",
       "\n",
       "   review_stars                                               text  \\\n",
       "0             3  The menu sounded promising, with over fifty di...   \n",
       "1             3  This wasn't the worst Chinese food but it wasn...   \n",
       "2             1  I have been coming to this restaurant for over...   \n",
       "3             5  My husband and I love this place.\\nGreat price...   \n",
       "4             5  Great place for beverages with your friends wh...   \n",
       "\n",
       "   amb_casual  amb_classy  amb_target  text_length  \\\n",
       "0         0.0         0.0           0          121   \n",
       "1         0.0         0.0           0           78   \n",
       "2         0.0         0.0           0           52   \n",
       "3         0.0         0.0           0           23   \n",
       "4         0.0         0.0           0           61   \n",
       "\n",
       "                                          clean_text  clean_text_length  \\\n",
       "0  menu sounded promising fifty different dishes ...                 68   \n",
       "1  wasn' worst chinese food wasn' best egg foo yo...                 41   \n",
       "2  coming restaurant 20 years purchased shrimp fr...                 27   \n",
       "3  husband love place great price lot food make s...                 13   \n",
       "4  great place beverages friends watch game lots ...                 43   \n",
       "\n",
       "                                     clean_text_stem  clean_text_stem_length  \n",
       "0  menu sound promis fifti differ dish differ sty...                      66  \n",
       "1  worst chines food best egg foo young dri overc...                      39  \n",
       "2  come restaur 20 year purchas shrimp fri rice g...                      26  \n",
       "3  husband love place great price lot food make s...                      12  \n",
       "4  great place beverag friend watch game lot tv g...                      43  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loads the data\n",
    "df = pd.read_csv('../../Data/reviews_stemmed_balanced.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up X and y\n",
    "X = df['clean_text_stem']\n",
    "y = df['amb_target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for the categorical y response variable \n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the data into training and test sets from sample\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    stratify=y, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8210     well final got around tri place like got meatl...\n",
       "12491    love mohawk atmospher food outstand bad thing ...\n",
       "55375    tri sever chines place powel worth mention twi...\n",
       "34871    boyfriend stop dinner first night drive across...\n",
       "17449    stop ice cream treat dinner main street love i...\n",
       "                               ...                        \n",
       "47219    great food comfort atmospher food great gorgeo...\n",
       "48987    bang buck price got awesom mediterrenean food ...\n",
       "62754    two year want tri bibibop 5th avenu gv locat f...\n",
       "25438    best servic small oper much staff partli accou...\n",
       "26916    chicken avocado sandwich favorit mine none foo...\n",
       "Name: clean_text_stem, Length: 48650, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the Counter function\n",
    "from collections import Counter\n",
    "\n",
    "# import the tokenizer from keras preprocessing \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a function that counts unique words\n",
    "def counter_word(text):\n",
    "    count = Counter()\n",
    "    for doc in text.values:\n",
    "        for word in doc.split():\n",
    "            count[word] += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8210     well final got around tri place like got meatl...\n",
       "12491    love mohawk atmospher food outstand bad thing ...\n",
       "55375    tri sever chines place powel worth mention twi...\n",
       "34871    boyfriend stop dinner first night drive across...\n",
       "17449    stop ice cream treat dinner main street love i...\n",
       "                               ...                        \n",
       "47219    great food comfort atmospher food great gorgeo...\n",
       "48987    bang buck price got awesom mediterrenean food ...\n",
       "62754    two year want tri bibibop 5th avenu gv locat f...\n",
       "25438    best servic small oper much staff partli accou...\n",
       "26916    chicken avocado sandwich favorit mine none foo...\n",
       "Name: clean_text_stem, Length: 48650, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'well final got around tri place like got meatloaf melt fri ok wish would got someth els honest want get anyth adventur diner first time meatloaf fri flat top grill end dri result fri averag hot tea great although littl thing impress special day got bite chicken homemad noodl veggi still littl crunchi dish well done usual like sort thing friend pasta must pretti good say jack shit gone kind picki eater oh one thing legal tender challeng friend cash'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts the number of times a unique word appears\n",
    "counter = counter_word(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28152"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finds the length or the number of unique words\n",
    "len(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define max number of words in a sequence \n",
    "* Setting this max number is important because we need to define a maximum sequence length that we can set to a number we pick \n",
    "* Note: Depending on the text, it is better to set this number high\n",
    "* (ex: Tweet - it is better to set this number to a high number between 50-70) \n",
    "* (ex: bigger text - you can set it to 200 or more) \n",
    "* In our trial, we're just going to start off small with 50 words\n",
    "\n",
    "The reason we need to define the sequence length is because when we use it with Tensorflow, we're going to need the same number of words/sequence length for each sequence. \n",
    "\n",
    "We won't be able to have sequences of different lengths. We need to map them to the same sequence size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = len(counter)\n",
    "\n",
    "# Max number of words in a sequence\n",
    "max_length = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the Tokenizer Class\n",
    "\n",
    "The next thing we need to use is the Tokenizer class from keras to tokenize the train sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the tokenizer from keras preprocessing \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the tokenizer onto the train sentences \n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulls the word index from the tokenizer \n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# This is an attribute of the tokenizer\n",
    "# The attribute is a dictionary where\n",
    "# key = actual word \n",
    "# value = the number that will now represent that word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the sequences from our tokenizer, based on the indices from the word_index\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'well final got around tri place like got meatloaf melt fri ok wish would got someth els honest want get anyth adventur diner first time meatloaf fri flat top grill end dri result fri averag hot tea great although littl thing impress special day got bite chicken homemad noodl veggi still littl crunchi dish well done usual like sort thing friend pasta must pretti good say jack shit gone kind picki eater oh one thing legal tender challeng friend cash'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_sequences[0]  # This sequence has 24 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now adding padding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "\n",
    "train_padded = pad_sequences(\n",
    "    train_sequences, maxlen=max_length, padding='post', truncating='post'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  29,  260,   20,  109,   13,    3,    7,   20, 1515,  671,   36,\n",
       "        227,  330,   14,   20,  118,  245, 1227,   37,    8,  234, 1531,\n",
       "        580,   45,    6, 1515,   36, 1170,   76,  314,  190,  357, 1622,\n",
       "         36,  426,   93,  276,    5,  416,   42,   67,  267,  122,   90,\n",
       "         20,  324,   16,  638,  480,  299], dtype=int32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_padded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the test dataset sequences and padding\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "test_padded = pad_sequences(\n",
    "    test_sequences, maxlen=max_length, padding='post', truncating='post'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "well final got around tri place like got meatloaf melt fri ok wish would got someth els honest want get anyth adventur diner first time meatloaf fri flat top grill end dri result fri averag hot tea great although littl thing impress special day got bite chicken homemad noodl veggi still littl crunchi dish well done usual like sort thing friend pasta must pretti good say jack shit gone kind picki eater oh one thing legal tender challeng friend cash\n",
      "[29, 260, 20, 109, 13, 3, 7, 20, 1515, 671, 36, 227, 330, 14, 20, 118, 245, 1227, 37, 8, 234, 1531, 580, 45, 6, 1515, 36, 1170, 76, 314, 190, 357, 1622, 36, 426, 93, 276, 5, 416, 42, 67, 267, 122, 90, 20, 324, 16, 638, 480, 299, 99, 42, 862, 89, 29, 316, 188, 7, 758, 67, 74, 403, 307, 53, 2, 68, 1796, 1807, 597, 195, 1364, 1344, 337, 11, 67, 4879, 427, 1787, 74, 1052]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.values[0])\n",
    "print(train_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'well final got around tri place like got meatloaf melt fri ok wish would got someth els honest want get anyth adventur diner first time meatloaf fri flat top grill end dri result fri averag hot tea great although littl thing impress special day got bite chicken homemad noodl veggi still littl crunchi dish well done usual like sort thing friend pasta must pretti good say jack shit gone kind picki eater oh one thing legal tender challeng friend cash'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checks to make sure that you can decode in reverse \n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "\n",
    "decode(train_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train (48650, 50)\n",
      "Shape of test (16217, 50)\n"
     ]
    }
   ],
   "source": [
    "# Checks the shape of the train and the shape of the test\n",
    "print(f'Shape of train {train_padded.shape}')\n",
    "print(f'Shape of test {test_padded.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the Network topology\n",
    "\n",
    "* We could've used One Hot Encoding (OHE) to convert these indices into vectors of 0s an 1s, but this would increase the dimensionality of our features \n",
    "\n",
    "**Instead...**\n",
    "* The Embedding layer - maps each word to a vector of a fixed size with real value elements...\n",
    "* In contrast to One Hot Encoding, we can use finite size vector to represent an infinite number of real numbers. \n",
    "* We're going to use dimensionality of this embedding layer (32) and the input length will be the max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up network topology \n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(num_words, 32, input_length=max_length))\n",
    "\n",
    "# LSTM layer\n",
    "model.add(LSTM(64))\n",
    "\n",
    "# Dense hidden layers\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=l2(.001)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(8, activation='relu', kernel_regularizer=l2(.001)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "optimizer = Adam(lr=3e-4)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 32)            900864    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 8)                 520       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 36        \n",
      "=================================================================\n",
      "Total params: 930,412\n",
      "Trainable params: 930,412\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "191/191 [==============================] - 36s 166ms/step - loss: 1.4455 - acc: 0.3072 - val_loss: 1.4037 - val_acc: 0.3287\n",
      "Epoch 2/25\n",
      "191/191 [==============================] - 39s 204ms/step - loss: 1.3917 - acc: 0.3186 - val_loss: 1.2772 - val_acc: 0.4142\n",
      "Epoch 3/25\n",
      "191/191 [==============================] - 30s 160ms/step - loss: 1.2642 - acc: 0.4058 - val_loss: 1.1575 - val_acc: 0.4824\n",
      "Epoch 4/25\n",
      "191/191 [==============================] - 28s 148ms/step - loss: 1.1768 - acc: 0.4551 - val_loss: 1.1261 - val_acc: 0.4770\n",
      "Epoch 5/25\n",
      "191/191 [==============================] - 31s 163ms/step - loss: 1.1237 - acc: 0.4789 - val_loss: 1.0857 - val_acc: 0.5037\n",
      "Epoch 6/25\n",
      "191/191 [==============================] - 30s 156ms/step - loss: 1.0790 - acc: 0.4988 - val_loss: 1.0673 - val_acc: 0.5068\n",
      "Epoch 7/25\n",
      "191/191 [==============================] - 51s 266ms/step - loss: 1.0413 - acc: 0.5158 - val_loss: 1.0604 - val_acc: 0.5113\n",
      "Epoch 8/25\n",
      "191/191 [==============================] - 61s 319ms/step - loss: 1.0040 - acc: 0.5386 - val_loss: 1.0533 - val_acc: 0.5410\n",
      "Epoch 9/25\n",
      "191/191 [==============================] - 61s 321ms/step - loss: 0.9816 - acc: 0.5521 - val_loss: 1.0435 - val_acc: 0.5523\n",
      "Epoch 10/25\n",
      "191/191 [==============================] - 59s 308ms/step - loss: 0.9603 - acc: 0.5630 - val_loss: 1.0650 - val_acc: 0.5576\n",
      "Epoch 11/25\n",
      "191/191 [==============================] - 64s 335ms/step - loss: 0.9334 - acc: 0.5767 - val_loss: 1.0708 - val_acc: 0.5286\n",
      "Epoch 12/25\n",
      "191/191 [==============================] - 64s 334ms/step - loss: 0.9253 - acc: 0.5823 - val_loss: 1.1006 - val_acc: 0.5632\n",
      "Epoch 13/25\n",
      "191/191 [==============================] - 61s 319ms/step - loss: 0.9013 - acc: 0.5965 - val_loss: 1.0923 - val_acc: 0.5687\n",
      "Epoch 14/25\n",
      "191/191 [==============================] - 58s 304ms/step - loss: 0.8881 - acc: 0.6009 - val_loss: 1.1227 - val_acc: 0.5645\n",
      "Epoch 15/25\n",
      "191/191 [==============================] - 58s 303ms/step - loss: 0.8637 - acc: 0.6209 - val_loss: 1.0951 - val_acc: 0.5746\n",
      "Epoch 16/25\n",
      "191/191 [==============================] - 59s 310ms/step - loss: 0.8528 - acc: 0.6272 - val_loss: 1.1386 - val_acc: 0.5835\n",
      "Epoch 17/25\n",
      "191/191 [==============================] - 58s 303ms/step - loss: 0.8309 - acc: 0.6396 - val_loss: 1.1448 - val_acc: 0.5856\n",
      "Epoch 18/25\n",
      "191/191 [==============================] - 61s 318ms/step - loss: 0.8179 - acc: 0.6446 - val_loss: 1.1133 - val_acc: 0.5811\n",
      "Epoch 19/25\n",
      "191/191 [==============================] - 59s 309ms/step - loss: 0.8440 - acc: 0.6352 - val_loss: 1.2241 - val_acc: 0.5895\n",
      "Epoch 20/25\n",
      "191/191 [==============================] - 58s 306ms/step - loss: 0.8005 - acc: 0.6574 - val_loss: 1.2359 - val_acc: 0.5848\n",
      "Epoch 21/25\n",
      "191/191 [==============================] - 57s 299ms/step - loss: 0.7840 - acc: 0.6646 - val_loss: 1.2373 - val_acc: 0.5886\n",
      "Epoch 22/25\n",
      "191/191 [==============================] - 59s 310ms/step - loss: 0.7701 - acc: 0.6719 - val_loss: 1.3163 - val_acc: 0.5882\n",
      "Epoch 23/25\n",
      "191/191 [==============================] - 55s 290ms/step - loss: 0.7771 - acc: 0.6654 - val_loss: 1.3085 - val_acc: 0.5889\n",
      "Epoch 24/25\n",
      "191/191 [==============================] - 59s 307ms/step - loss: 0.7676 - acc: 0.6765 - val_loss: 1.3069 - val_acc: 0.5886\n",
      "Epoch 25/25\n",
      "191/191 [==============================] - 59s 308ms/step - loss: 0.7575 - acc: 0.6755 - val_loss: 1.3206 - val_acc: 0.5849\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history = model.fit(\n",
    "    train_padded, y_train, epochs=25, batch_size=256, \n",
    "    validation_data=(test_padded, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/my_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/my_model/assets\n"
     ]
    }
   ],
   "source": [
    "# Saves the model\n",
    "model.save('saved_model/my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the model\n",
    "model = tf.keras.models.load_model('saved_model/my_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the results on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16217"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs = model.predict(test_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16217"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_class = (pred_probs > 0.5).astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_class[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5650227304036368\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1score=f1_score(preds_class, y_test, average='micro')\n",
    "print(f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       ...,\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 1, 0]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       ...,\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 1, 0]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('../../Data/X_train_to_tokenize.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8210     well final got around tri place like got meatl...\n",
       "12491    love mohawk atmospher food outstand bad thing ...\n",
       "55375    tri sever chines place powel worth mention twi...\n",
       "34871    boyfriend stop dinner first night drive across...\n",
       "17449    stop ice cream treat dinner main street love i...\n",
       "                               ...                        \n",
       "47219    great food comfort atmospher food great gorgeo...\n",
       "48987    bang buck price got awesom mediterrenean food ...\n",
       "62754    two year want tri bibibop 5th avenu gv locat f...\n",
       "25438    best servic small oper much staff partli accou...\n",
       "26916    chicken avocado sandwich favorit mine none foo...\n",
       "Name: clean_text_stem, Length: 48650, dtype: object"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m65"
  },
  "kernelspec": {
   "display_name": "Python (nnet)",
   "language": "python",
   "name": "nnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
